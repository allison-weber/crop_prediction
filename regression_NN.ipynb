{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPUBiH+e7oYyVjcidtjzYoU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Simple NN Regression Model\n","**Goal**: Use metrological data for the year to predict the crop yield for that year.\n","\n","**Run instructions**:\n","- in collab, change ``GOOGLE_DRIVE_PATH`` to appropriate directory structure (marked with ``# TODO``)\n","- select crop to predict yield for by changing crop variable in data download section (marked with ``# TODO``)\n","- *optional*: update hyperparameters in section ``# Set hyper-parameters``\n","- then just select 'Run all', results + plots will print at bottom\n","\n","**Possible architecture improvments**:\n","- Add more layers to simple NN\n","- Try transformer architecture\n","\n","**Future code improvments**:\n","- Add Torchvision experiment tracking, etc.\n","- Model weight visualization- heat map to see which meterological factors in which month are most important to each crop.\n","- Refactor code: functionalize hyper-parameter tuning, save best performing-model, etc.\n","\n","**Improvements attempted**\n","1. Combine all crops into 1 data set with dummy variables for crop type => intial results worse (lower RMSE, ~26 without tuning)\n","2. Add ``asd_desc`` which describes the type of land in each county (make dummy variables, etc.) => helps very slightly (e.g. for Corn, RMSE goes down  1, to ~15 from ~16)\n","3. fine-tune hyperparameters (learning rate, hidden dimensions, batch size, weight decay, etc.) => random search better + faster than grid but neither much better than intuition (grid search actually worse when only given 10 epochs instead of 20)\n","  - to run grid search, set ```do_grid_search=true```\n","  - to run random search, set ```NUM_SEARCH``` to postive integer (e.g. 10)"],"metadata":{"id":"eoo_eiZ67iz7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OgrKmSxklemx"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","GOOGLE_DRIVE_PATH_POST_MYDRIVE = 'DL/MMST-ViT-main'  # TODO change this\n","GOOGLE_DRIVE_PATH = os.path.join('/content', 'drive', 'MyDrive', GOOGLE_DRIVE_PATH_POST_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"metadata":{"id":"ekVdldtvlnkt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","if 'google.colab' in sys.modules:\n","  print(f'Running in google colab. Our path is `{GOOGLE_DRIVE_PATH}`')\n","else:\n","  GOOGLE_DRIVE_PATH = '.'\n","  print('Running locally.')"],"metadata":{"id":"s0UTwe3_l4Ni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import ParameterGrid\n","\n","# Pytorch package\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from typing import Tuple, Dict, List\n","\n","# Tqdm progress bar\n","from tqdm import tqdm_notebook, tqdm\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"You are using device: %s\" % device)"],"metadata":{"id":"AJmXWkIql_ZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","np.random.seed(seed)"],"metadata":{"id":"2yeuLya6mDjQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load data - 1 crop\n","crop = 'Cotton' # TODO set as appropriate. Options= Corn, Cotton, Soybeans, WinterWheat\n","# 'region' will be 1 crop but includes include dummy columns for region,\n","# 'all_crops' has all 4 crops (identified by dummy cols) in 1 dataset\n","# default is 1 crop without dummy columns for region\n","dataset = True # TODO set as desired. Options = 'region', 'all_crops', 'default'\n","\n","if dataset == 'region':\n","    df = pd.read_csv(f'{GOOGLE_DRIVE_PATH}/simple_model/monthly_{crop}_with_region.csv')\n","elif dataset == 'all_crops':\n","    df = pd.read_csv(f'{GOOGLE_DRIVE_PATH}/simple_model/all_crops.csv')\n","else:\n","  df = pd.read_csv(f'{GOOGLE_DRIVE_PATH}/simple_model/monthly_{crop}_model_data.csv')\n","\n","# # to load 1 dataset with all 4 crops, identified by dummy columns\n","# df = pd.read_csv(f'{GOOGLE_DRIVE_PATH}/simple_model/all_crops.csv')\n","\n","df.head()\n","X = df.drop(columns='yield').to_numpy()\n","y = df['yield'].to_numpy()\n","\n","num_data_features = X.shape[1]\n","X.shape, y.shape, num_data_features"],"metadata":{"id":"2gZE0TyzmdNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train-test split of the dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True)\n","\n","# Standardizing data\n","scaler = StandardScaler()\n","scaler.fit(X_train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n","\n","print(X_train.shape, y_train.shape)\n","print(X_test.shape, y_test.shape)\n","print(f'{X_train[0].shape=}')\n","print(f'{y_train[0]=}')"],"metadata":{"id":"T9sAhyaumYL4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# turn into correct type format to be put into data loader\n","# based on pytorch docs\n","class CustomDataset(Dataset):\n","    def __init__(self, X_p, y_p, transform=None, target_transform=None):\n","        self.X = X_p\n","        self.y = y_p\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        item = self.X[idx]\n","        label = self.y[idx]\n","        if self.transform:\n","            item = self.transform(item)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return item, label\n","\n","train_data = CustomDataset(X_train, y_train)\n","test_data = CustomDataset(X_test, y_test)"],"metadata":{"id":"2z-uGFkamcl0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","# Setup batch size and number of workers\n","BATCH_SIZE = 4\n","NUM_WORKERS = os.cpu_count()\n","\n","train_dataloader = DataLoader(dataset=train_data, # use custom created train Dataset\n","                                     batch_size=BATCH_SIZE, # how many samples per batch?\n","                                     num_workers=NUM_WORKERS, # how many subprocesses to use for data loading? (higher = more)\n","                                     shuffle=True) # shuffle the data?\n","\n","test_dataloader = DataLoader(dataset=test_data, # use custom created test Dataset\n","                                    batch_size=BATCH_SIZE,\n","                                    num_workers=NUM_WORKERS,\n","                                    shuffle=False) # don't usually need to shuffle testing data\n","\n","train_dataloader, test_dataloader\n","next(iter(train_dataloader))\n","# # check ok\n","item, label = next(iter(train_dataloader))\n","print(f\"Data shape: {item.shape} -> [batch_size, # features]\")\n","print(f\"Label shape: {label.shape}\")"],"metadata":{"id":"9kTv3C57oEnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyModel(nn.Module):\n","    \"\"\"\n","    Model architecture is just basic to test code\n","    \"\"\"\n","    def __init__(self, input_features: int, hidden_units: int, act_func=nn.ReLU()) -> None:\n","        super().__init__()\n","        self.linear_layer_stack = nn.Sequential(\n","            nn.Linear(in_features=input_features, out_features=hidden_units),\n","            act_func,\n","            # nn.Linear(in_features=hidden_units, out_features=hidden_units),\n","            # act_func,\n","            nn.Linear(in_features=hidden_units, out_features=1)\n","        )\n","\n","    def forward(self, x):\n","        return self.linear_layer_stack(x)"],"metadata":{"id":"9Wtex8WFzWY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training code adapted from: https://www.learnpytorch.io/\n","\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer):\n","    # Put model in train mode\n","    model.train()\n","\n","    # Setup train loss and train accuracy values\n","    train_loss, train_acc = 0, 0\n","\n","    # Loop through data loader data batches\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Send data to target device\n","        X, y = X.to(device), y.to(device)\n","\n","        # 1. Forward pass\n","        y_pred = model(X)\n","\n","        # 2. Calculate  and accumulate loss\n","        loss = loss_fn(y_pred, y)\n","        train_loss += loss.item()\n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    train_loss = train_loss / len(dataloader)\n","\n","    return train_loss"],"metadata":{"id":"4ufdAMOM0cQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training code adapted from: https://www.learnpytorch.io/\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module):\n","    # Put model in eval mode\n","    model.eval()\n","\n","    # Setup test loss and test accuracy values\n","    test_loss, test_acc = 0, 0\n","\n","    # Turn on inference context manager\n","    with torch.inference_mode():\n","        # Loop through DataLoader batches\n","        for batch, (X, y) in enumerate(dataloader):\n","            # Send data to target device\n","            X, y = X.to(device), y.to(device)\n","\n","            # 1. Forward pass\n","            test_pred_logits = model(X)\n","\n","            # 2. Calculate and accumulate loss\n","            loss = loss_fn(test_pred_logits, y)\n","            test_loss += loss.item()\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    test_loss = test_loss / len(dataloader)\n","    return test_loss"],"metadata":{"id":"PYonSudz0dTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training code adapted from: https://www.learnpytorch.io/\n","from tqdm.auto import tqdm\n","\n","# 1. Take in various parameters required for training and test steps\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n","          epochs: int = 5):\n","\n","    # 2. Create empty results dictionary\n","    results = {\"train_loss\": [],\n","        \"test_loss\": [],\n","    }\n","\n","    # 3. Loop through training and testing steps for a number of epochs\n","    for epoch in tqdm(range(epochs)):\n","        train_loss = train_step(model=model,\n","                                           dataloader=train_dataloader,\n","                                           loss_fn=loss_fn,\n","                                           optimizer=optimizer)\n","        test_loss = test_step(model=model, dataloader=test_dataloader, loss_fn=loss_fn)\n","\n","        # # 4. Print out what's happening\n","        # if epoch % 5 == 0 or epoch == epochs - 1:\n","        print(\n","            f\"Epoch: {epoch} | \"\n","            f\"train_loss: {train_loss:.4f} | \"\n","            f\"test_loss: {test_loss:.4f}\"\n","        )\n","\n","        # 5. Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"test_loss\"].append(test_loss)\n","\n","    # 6. Return the filled results at the end of the epochs\n","    return results"],"metadata":{"id":"NP-wbnNe0lFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# based on: https://gist.github.com/jamesr2323/33c67ba5ac29880171b63d2c7f1acdc5\n","class RMSELoss(torch.nn.Module):\n","    def __init__(self):\n","        super(RMSELoss,self).__init__()\n","\n","    def forward(self,x,y):\n","        criterion = nn.MSELoss()\n","        eps = 1e-6\n","        loss = torch.sqrt(criterion(x, y) + eps)\n","        return loss"],"metadata":{"id":"nkXJI8Pk4M5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting code adapted from: https://www.learnpytorch.io/\n","import matplotlib.pyplot as plt\n","\n","def plot_loss_curves(results: Dict[str, List[float]]):\n","    \"\"\"Plots training curves of a results dictionary.\n","\n","    Args:\n","        results (dict): dictionary containing list of values, e.g.\n","            {\"train_loss\": [...],\n","             \"test_loss\": [...],\n","    \"\"\"\n","\n","    # Get the loss values of the results dictionary (training and test)\n","    loss = results['train_loss']\n","    test_loss = results['test_loss']\n","\n","    # Figure out how many epochs there were\n","    epochs = range(len(results['train_loss']))\n","\n","    # Setup a plot\n","    plt.figure(figsize=(15, 7))\n","\n","    # Plot loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, loss, label='train_loss')\n","    plt.plot(epochs, test_loss, label='test_loss')\n","    plt.title(f'Loss: {crop}')\n","    plt.xlabel('Epochs')\n","    plt.legend()"],"metadata":{"id":"ynP_gOP853cS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set random seeds\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","\n","# Set hyper-parameters\n","NUM_EPOCHS = 20\n","HIDDEN_DIM = 45 # corn: 80, cotton: 64, soybeans:80/55 winterwheat 80/65\n","learning_rate = 0.01 # corn: 0.001, cotton: 0.01 , soybeans 0.005, winterwheat  0.05\n","weight_decay= 0.05 # corn: 0.01, cotton: 0.1, spybeans: 0.01, winterwheat 0.05\n","activation_func = nn.LeakyReLU() #corn: nn.LeakyReLU(), cotton: nn.SELU(), soybeans: nn.ReLU()\n","# Setup model\n","model_0 = MyModel(num_data_features, HIDDEN_DIM, activation_func).to(device)\n","\n","# Setup loss function and optimizer\n","loss_fn = RMSELoss() # nn.MSELoss()\n","optimizer = torch.optim.Adam(params=model_0.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","# Start the timer\n","from timeit import default_timer as timer\n","start_time = timer()\n","\n","# Train model_0\n","model_0_results = train(model=model_0,\n","                        train_dataloader=train_dataloader,\n","                        test_dataloader=test_dataloader,\n","                        optimizer=optimizer,\n","                        loss_fn=loss_fn,\n","                        epochs=NUM_EPOCHS)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n","plot_loss_curves(model_0_results)"],"metadata":{"id":"CEIRQloM0zGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GRID SEARCH\n","do_grid_search = False\n","NUM_EPOCHS = 10\n","activation_func = nn.LeakyReLU()\n","\n","hyperparam_grid = {\n","    'HIDDEN_DIM': [20, 64, 100],\n","    'learning_rate': [1e-2, 1e-3, 5e-3, 1e-4],\n","    'weight_decay': [0.1, 0.01, 0.05, 0.001],\n","    # 'activation_func': [nn.LeakyReLU(), nn.ReLU(), nn.SELU(), nn.GELU()]\n","}\n","\n","if do_grid_search:\n","  best_rmse = float('inf')\n","  best_config = None\n","  best_rmse_last_epoch = float('inf')\n","  best_config_last_epoch = None\n","  for config in ParameterGrid(hyperparam_grid):\n","    print(f'Training with config: {config}')\n","    model_0 = MyModel(num_data_features, config['HIDDEN_DIM'], activation_func).to(device)\n","\n","    # Setup loss function and optimizer\n","    loss_fn = RMSELoss() # nn.MSELoss()\n","    optimizer = torch.optim.Adam(params=model_0.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n","\n","    # Start the timer\n","    from timeit import default_timer as timer\n","    start_time = timer()\n","\n","    # Train model_0\n","    model_0_results = train(model=model_0,\n","                            train_dataloader=train_dataloader,\n","                            test_dataloader=test_dataloader,\n","                            optimizer=optimizer,\n","                            loss_fn=loss_fn,\n","                            epochs=NUM_EPOCHS)\n","    final_rmse = model_0_results['test_loss'][-1]\n","    min_rmse = min(model_0_results['test_loss'])\n","    if min_rmse < best_rmse:\n","        best_rmse = min_rmse\n","        best_config = config\n","    if final_rmse < best_rmse_last_epoch:\n","        best_rmse_last_epoch = final_rmse\n","        best_config_last_epoch = config\n","\n","  print(f'Best RMSE: {best_rmse}, Best Config: {best_config}')\n","  print(f'Best RMSE at last epoch: {best_rmse_last_epoch}, Best Config at last epoch: {best_config_last_epoch}')\n","else:\n","  print('Set train_grid=True if you want to run the grid search. See cell above for results of 1 model.')\n"],"metadata":{"id":"rpbm5YIXCqLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RANDOM SEARCH\n","# If set NUM_SEARCH to 0, won't run\n","NUM_SEARCH = 30\n","\n","NUM_EPOCHS = 20\n","learning_rate_range = result = [x / 10000 for x in range(1, 1000, 10)]\n","weight_decay_rate_range = result = [x / 100000 for x in range(1, 10000, 10)]\n","hidden_dim_range = result = range(20, 100, 5)\n","activation_functions = [nn.LeakyReLU(), nn.ReLU(), nn.SELU(), nn.GELU()]\n","\n","best_rmse = float('inf')\n","best_config = None\n","best_rmse_last_epoch = float('inf')\n","best_config_last_epoch = None\n","\n","for i in range(NUM_SEARCH):\n","  learning_rate = random.choice(learning_rate_range)\n","  weight_decay = random.choice(weight_decay_rate_range)\n","  hidden_dim = random.choice(hidden_dim_range)\n","  activation_func = random.choice(activation_functions)\n","  config = {\"learning_rate\": learning_rate, 'weight_decay': weight_decay,\n","            'hidden_dim': hidden_dim, 'activation_func': activation_func }\n","  print(f\"Run {i}: config is {config}\")\n","\n","  # Setup model\n","  model_0 = MyModel(num_data_features, hidden_dim, activation_func).to(device)\n","\n","  # Setup loss function and optimizer\n","  loss_fn = RMSELoss() # nn.MSELoss()\n","  optimizer = torch.optim.Adam(params=model_0.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","  # Train model_0\n","  model_0_results = train(model=model_0,\n","                          train_dataloader=train_dataloader,\n","                          test_dataloader=test_dataloader,\n","                          optimizer=optimizer,\n","                          loss_fn=loss_fn,\n","                          epochs=NUM_EPOCHS)\n","\n","  print(model_0_results)\n","\n","  final_rmse = model_0_results['test_loss'][-1]\n","  min_rmse = min(model_0_results['test_loss'])\n","  if min_rmse < best_rmse:\n","      best_rmse = min_rmse\n","      best_config = config\n","  if final_rmse < best_rmse_last_epoch:\n","      best_rmse_last_epoch = final_rmse\n","      best_config_last_epoch = config\n","\n","print(f'Best RMSE: {best_rmse}, Best Config: {best_config}')\n","print(f'Best RMSE at last epoch: {best_rmse_last_epoch}, Best Config at last epoch: {best_config_last_epoch}')\n","\n","\n","\n"],"metadata":{"id":"7aBS14cylBcZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Summary stats for paper')\n","print(f'{crop=}, {min(y)=}, {max(y)=}, {np.mean(y)=}, {np.std(y)=}')\n","corn = pd.read_csv(f'{GOOGLE_DRIVE_PATH}/simple_model/monthly_Corn_with_region.csv')\n","print(f'{corn.shape=}')\n","corn = pd.read_csv(f'{GOOGLE_DRIVE_PATH}/simple_model/monthly_Corn_model_data.csv')\n","print(f'{corn.shape=}')\n","print(f\"Number region cols: {len([col for col in corn.columns if 'asd_desc' in col])}\")"],"metadata":{"id":"OhgVdY_ZZmZo"},"execution_count":null,"outputs":[]}]}